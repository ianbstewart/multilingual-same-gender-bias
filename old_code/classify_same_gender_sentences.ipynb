{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify same-gender sentences\n",
    "Let's try to classify same-gender vs. different-gender sentences and then identify matching sentences in same corpus data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# extract sentences that mention relationships\n",
    "import bz2\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from data_helpers import load_relationship_occupation_template_data\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "PUNCT_MATCHERS = [\n",
    "    [re.compile('&apos;'), \"'\"],\n",
    "]\n",
    "def clean_web_text(text):\n",
    "    clean_text = text.strip()\n",
    "    for m_i, w_i in PUNCT_MATCHERS:\n",
    "        clean_text = m_i.sub(w_i, clean_text)\n",
    "    # remove HTML junk\n",
    "    text_soup = BeautifulSoup(clean_text)\n",
    "    clean_text = text_soup.text\n",
    "    return clean_text\n",
    "occupation_words, relationship_word_data, relationship_sents, langs, lang_art_PRON_lookup, lang_POSS_PRON_lookup = load_relationship_occupation_template_data()\n",
    "langs = ['it']\n",
    "subject_genders = ['male', 'female']\n",
    "relationship_sents = []\n",
    "for lang_i in langs:\n",
    "    relationship_phrases_i = []\n",
    "    for gender_j in subject_genders:\n",
    "        # possessive pronoun + noun e.g. \"sua moglie\"\n",
    "# #         pron_j = lang_POSS_PRON_lookup[lang_i][gender_j]\n",
    "# #         # remove \"il\"/\"la\" for IT pron\n",
    "# #         if(lang_i == 'it'):\n",
    "# #             pron_j = pron_j.split(' ')[-1]\n",
    "#         relationship_phrases_j = relationship_word_data.loc[:, f'{lang_i}_{gender_j}'].apply(lambda x: f'{pron_j} {x}')\n",
    "        # normal relationship words e.g. \"la moglie\" => looking for possessors \"la moglie del generale\"\n",
    "        relationship_phrases_j = relationship_word_data.loc[:, f'{lang_i}_{gender_j}']\n",
    "        relationship_phrases_i.extend(relationship_phrases_j.values.tolist())\n",
    "    relationship_phrase_matcher_i = re.compile('|'.join(relationship_phrases_i))\n",
    "    file_name_i = f'data/wiki/{lang_i}wiki-20181001-corpus.xml.bz2'\n",
    "    matching_sents_i = []\n",
    "    for l in tqdm(bz2.open(file_name_i, 'rt')):\n",
    "        l = clean_web_text(l)\n",
    "        l_sents = sent_tokenize(l)\n",
    "        for sent_j in l_sents:\n",
    "            relationship_phrase_search_l = relationship_phrase_matcher_i.search(sent_j.lower())\n",
    "            if(relationship_phrase_search_l is not None):\n",
    "                matching_sents_i.append([relationship_phrase_search_l.group(0), sent_j])\n",
    "    matching_sents_i = pd.DataFrame(matching_sents_i, columns=['relationship_word', 'sent'])\n",
    "    matching_sents_i = matching_sents_i.assign(**{'lang' : lang_i})\n",
    "    relationship_sents.append(matching_sents_i)\n",
    "relationship_sents = pd.concat(relationship_sents, axis=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "display(relationship_sents.loc[:, 'relationship_word'].value_counts())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## save to file!!\n",
    "# relationship_sents.to_csv('data/wiki/relationship_sent_data.gz', sep='\\t', compression='gzip', index=False) # possessive PRONOUN + phrase\n",
    "relationship_sents.to_csv('data/wiki/relationship_words_sent_data.gz', sep='\\t', compression='gzip', index=False) # single word"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "## reload\n",
    "import pandas as pd\n",
    "# relationship_sents = pd.read_csv('data/wiki/relationship_sent_data.gz', sep='\\t')  # possessive PRONOUN + phrase\n",
    "relationship_sents = pd.read_csv('data/wiki/relationship_words_sent_data.gz', sep='\\t')  # single word\n",
    "display(relationship_sents.head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## example sentences\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "for lang_i, data_i in relationship_sents.groupby('lang'):\n",
    "    for word_j, data_j in data_i.groupby('relationship_word'):\n",
    "        print(f'word = {word_j}')\n",
    "        display(data_j.loc[:, ['sent']].head(5))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get subject nouns for relationship target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we filter to sentences that have the relationship target as dependent on the subject?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "## reload\n",
    "import pandas as pd\n",
    "# relationship_sent_output_data = pd.read_csv('data/wiki/relationship_sent_data.gz', sep='\\t')\n",
    "relationship_sent_output_data = pd.read_csv('data/wiki/relationship_words_sent_data.gz', sep='\\t')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "import spacy\n",
    "# nlp_pipeline = spacy.load('it_core_news_sm') # small parser sucks!!\n",
    "nlp_pipeline = spacy.load('it_core_news_lg') # use bigger parser when possible!!"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "source": [
    "def find_phrase_head(sent, phrase):\n",
    "    ## TODO: restrict to extremely simple sentences like \n",
    "    # \"il generale e sua moglie mangiano\"\n",
    "    ## i.e. head noun should be (1) close to target and (2) be either subject or object of sentence\n",
    "    phrase_tokens = phrase.split(' ')\n",
    "#     sent_phrase_tokens = []\n",
    "    phrase_head = None\n",
    "    for i in range(len(sent)-len(phrase_tokens)):\n",
    "        if(all([sent[i+j].text==phrase_tokens[j] for j in range(len(phrase_tokens))])):\n",
    "            sent_phrase_tokens = sent[i:(i+len(phrase_tokens))]\n",
    "            # tmp debug\n",
    "#             print(f'phrase tokens = {sent_phrase_tokens}; POS = {[x.pos_ for x in sent_phrase_tokens]}')\n",
    "            # get head noun\n",
    "            phrase_nouns = list(filter(lambda x: x.pos_ == 'NOUN', sent_phrase_tokens))\n",
    "            if(len(phrase_nouns) > 0):\n",
    "                head_noun = phrase_nouns[0]\n",
    "                # get other noun in conjunction\n",
    "                if(head_noun.dep_ == 'conj'):\n",
    "                    phrase_head = head_noun.head\n",
    "                    break\n",
    "    return phrase_head\n",
    "import re\n",
    "possessor_word_matcher_lookup = {\n",
    "    'it' : re.compile('|'.join(['de', 'di', 'del', 'da', 'della']))\n",
    "}\n",
    "def is_token_connected_to_possessor(token, possessor_word_matcher):\n",
    "    token_children = list(token.children)\n",
    "#     print(f'children = {token_children}')\n",
    "    possessor_children = list(filter(lambda x: possessor_word_matcher.match(x.text.lower()) is not None, token_children))\n",
    "    return len(possessor_children)\n",
    "def find_phrase_possessor(sent, phrase_word, lang):\n",
    "    ## TODO: make it even more strict => \"the X of Y\" where \"Y\" is parent of \"of\"\n",
    "    # get possessor of phrase via \"nmod\"\n",
    "#     print(f'sent = {[x for x in sent]}; phrase_word = {phrase_word}')\n",
    "    phrase_token_matches = list(filter(lambda x: x.text.lower() == phrase_word, sent))\n",
    "    possessor = None\n",
    "    if(len(phrase_token_matches) > 0):\n",
    "        phrase_token = phrase_token_matches[0]\n",
    "        # look for source noun with NMOD dep and possessor child\n",
    "        possessor_word_matcher = possessor_word_matcher_lookup[lang]\n",
    "        phrase_children = list(filter(lambda x: x.dep_=='nmod' and is_token_connected_to_possessor(x, possessor_word_matcher), phrase_token.children))\n",
    "        if(len(phrase_children) > 0):\n",
    "            possessor = phrase_children[0]\n",
    "    return possessor\n",
    "## phrase head test\n",
    "# phrase = 'sua moglie'\n",
    "# sent = 'la donna e sua moglie sono andate al negozio'\n",
    "# sent = \"\"\"weihe nacque il 30 gennaio 1779 a mennighüffen, il secondo dei dodici figli di karl justus weihe (1752-1829), un pastore, e di sua moglie anna (nata rebeker).\"\"\"\n",
    "# should connect \"moglie\" to \"pastore\"\n",
    "# sent = nlp_pipeline(sent)\n",
    "# print(sent)\n",
    "# print(find_phrase_head(sent, phrase))\n",
    "## possessor test\n",
    "phrase_word = 'moglie'\n",
    "sent = 'era la moglie de la abogada'\n",
    "lang = 'it'\n",
    "# sent = 'terza moglie Isabelita Perón'\n",
    "sent = nlp_pipeline(sent)\n",
    "print(find_phrase_possessor(sent, phrase_word, lang))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "# relationship_sent_output_data = relationship_sent_output_data.assign(**{\n",
    "#     'sent_parse' : relationship_sent_output_data.loc[:, 'sent'].progress_apply(nlp_pipeline)\n",
    "# })\n",
    "## attempt to get phrase head => trash\n",
    "# relationship_sent_output_data = relationship_sent_output_data.assign(**{\n",
    "#     'relationship_word_source' : relationship_sent_output_data.apply(lambda x: find_phrase_head(x.loc['sent_parse'], x.loc['relationship_word']), axis=1)\n",
    "# })\n",
    "relationship_sent_output_data = relationship_sent_output_data.assign(**{\n",
    "    'relationship_word_source' : relationship_sent_output_data.progress_apply(lambda x: find_phrase_possessor(x.loc['sent_parse'], x.loc['relationship_word']), axis=1)\n",
    "})\n",
    "# get article => approximate gender\n",
    "# import re\n",
    "# male_articles = ['il', 'un', 'uno', 'del', 'dello']\n",
    "# female_articles = ['la', \"l'\", 'una', \"un'\", 'da', 'della', \"dell'\"]\n",
    "# articles = male_articles + female_articles\n",
    "# article_matcher = re.compile('|'.join(articles))\n",
    "# relationship_sent_output_data = relationship_sent_output_data.assign(**{\n",
    "#     'relationship_word_head_article' : relationship_sent_output_data.loc[:, 'relationship_word_head'].apply(lambda x: list(filter(lambda y: article_matcher.match(y.text) is not None, x.children)) if x is not None else [])\n",
    "# })\n",
    "# relationship_sent_output_data = relationship_sent_output_data.assign(**{\n",
    "#     'relationship_word_head_article' : relationship_sent_output_data.loc[:, 'relationship_word_head_article'].apply(lambda x: x[0] if len(x) > 0 else None)\n",
    "# })\n",
    "# get actual gender from morphology\n",
    "relationship_sent_output_data = relationship_sent_output_data.assign(**{\n",
    "    'relationship_word_source_gender' : relationship_sent_output_data.loc[:, 'relationship_word_source'].apply(lambda x: x.morph.get('Gender')[0] if x is not None and len(x.morph.get('Gender')) > 0 else None)\n",
    "})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "relationship_with_source_sent_data = relationship_sent_output_data[relationship_sent_output_data.loc[:, 'relationship_word_source'].apply(lambda x: x is not None)]\n",
    "# fix gender labels\n",
    "gender_lookup = {\n",
    "    'Masc' : 'male',\n",
    "    'Fem' : 'female'\n",
    "}\n",
    "relationship_with_source_sent_data = relationship_with_source_sent_data.assign(**{'relationship_word_source_gender' : relationship_with_source_sent_data.loc[:, 'relationship_word_source_gender'].apply(gender_lookup.get)})\n",
    "display(relationship_with_source_sent_data.loc[:, ['sent', 'relationship_word', 'relationship_word_source', 'relationship_word_source_gender']])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use this approximation to find same-gender and different-gender sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "source": [
    "## have to get relationship word gender\n",
    "from data_helpers import load_relationship_occupation_template_data\n",
    "occupation_words, relationship_words, relationship_sents, langs, lang_art_PRON_lookup, lang_POSS_PRON_lookup = load_relationship_occupation_template_data()\n",
    "# lookup gender\n",
    "langs = ['en', 'es', 'fr', 'it']\n",
    "genders = ['male', 'female']\n",
    "relationship_word_gender_lookup = {\n",
    "    l : {g : relationship_words.loc[:, f'{l}_{g}'].values for g in genders}\n",
    "    for l in langs\n",
    "}\n",
    "relationship_word_gender_lookup = {\n",
    "    k : {v : k1 for k1, v1 in v.items() for v in v1}\n",
    "    for k,v in relationship_word_gender_lookup.items()\n",
    "}\n",
    "## look up gender, compare w/ head noun gender, etc\n",
    "relationship_with_source_sent_data = relationship_with_source_sent_data.assign(**{\n",
    "    'relationship_word_gender' : relationship_with_source_sent_data.apply(lambda x: relationship_word_gender_lookup[x.loc['lang']][x.loc['relationship_word'].split(' ')[-1]], axis=1) \n",
    "})\n",
    "word_gender_vars = ['relationship_word_gender', 'relationship_word_source_gender']\n",
    "print(relationship_with_source_sent_data.loc[:, word_gender_vars].value_counts())\n",
    "# normalize by total\n",
    "display(relationship_with_source_sent_data.groupby('relationship_word_gender').apply(lambda x: x.loc[:, 'relationship_word_source_gender'].value_counts() / x.loc[:, 'relationship_word_source_gender'].value_counts().sum()).reset_index().pivot(index='relationship_word_gender', columns=['level_1'], values='relationship_word_source_gender').sort_index())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skew is actually less bad than I thought. Let's look at some example phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "source": [
    "for relationship_word_i, data_i in relationship_with_source_sent_data.groupby('relationship_word'):\n",
    "    print(f'*** relationship word = {relationship_word_i} ***')\n",
    "    relationship_word_gender_i = data_i.loc[:, 'relationship_word_gender'].iloc[0]\n",
    "    for gender_j, data_j in data_i.groupby('relationship_word_source_gender'):\n",
    "        if(gender_j == relationship_word_gender_i):\n",
    "            print(f'** gender = same-gender **')\n",
    "        else:\n",
    "            print(f'** gender = diff-gender **')\n",
    "        display(data_j.loc[:, ['sent', 'relationship_word_source']].head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these relationship source words are not people-related, e.g. `il fidanzato di bronzo` where `bronzo` is not a person.\n",
    "\n",
    "Let's filter these relationship word source words to only have person-related words, using [multilingual Wordnet](https://www.nltk.org/howto/wordnet.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## import nltk\n",
    "## nltk.download('omw')\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "person_category_matcher = re.compile('person.n.01')\n",
    "lemma_num_matcher = re.compile('(?<=\\.n\\.)(\\d+)(?=\\.)')\n",
    "wordnet_lang_lookup = {\n",
    "    'es' : 'spa',\n",
    "    'fr' : 'fra',\n",
    "    'it' : 'ita',\n",
    "}\n",
    "def is_word_a_person(word, lang):\n",
    "    wordnet_lang = wordnet_lang_lookup[lang]\n",
    "    word_is_person = False\n",
    "    # assume capital letter => name => person\n",
    "    if(word.istitle()):\n",
    "        word_is_person = True\n",
    "    else:\n",
    "    #     print(f'word type={type(word)}')\n",
    "        word_lemmas = wordnet.lemmas(word, lang=wordnet_lang)\n",
    "#         print(f'word={word}; lemmas={word_lemmas}')\n",
    "        # find main word sense\n",
    "        # sort lemmas by number: lower number => more \"core\" meaning\n",
    "        word_lemma_nums = list(map(lambda x: int(lemma_num_matcher.search(str(x)).group(0)) if lemma_num_matcher.search(str(x)) is not None else np.inf, word_lemmas))\n",
    "        if(len(word_lemma_nums) > 0):\n",
    "            max_word_lemma_num = min(word_lemma_nums)\n",
    "            main_lemmas = [x for x,y in zip(word_lemmas, word_lemma_nums) if y==max_word_lemma_num]\n",
    "            # best case: match lemma name w/ weird format e.g. \"donna.n.8.donna\"\n",
    "            word_lemma_matcher = re.compile(f'Lemma\\(\\'({word})\\.n.+')\n",
    "            if(len(main_lemmas) > 1):\n",
    "                word_match_main_lemma = list(filter(lambda x: word_lemma_matcher.match(str(x)), main_lemmas))\n",
    "                if(len(word_match_main_lemma) > 0):\n",
    "                    main_lemma = word_match_main_lemma[0]\n",
    "                else:\n",
    "                    main_lemma = main_lemmas[0]\n",
    "                # get hypernyms for main lemma\n",
    "                main_lemma_hypernym_paths = main_lemma.synset().hypernym_paths()\n",
    "                main_lemma_main_path = main_lemma_hypernym_paths[0]\n",
    "                path_contains_person_category = any(map(lambda x: person_category_matcher.match(x.name()), main_lemma_main_path))\n",
    "                if(path_contains_person_category):\n",
    "                    word_is_person = True\n",
    "    return word_is_person\n",
    "\n",
    "lang = 'it'\n",
    "word = 'donna'\n",
    "assert is_word_a_person(word, lang)\n",
    "word = 'cane'\n",
    "assert not is_word_a_person(word, lang)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "relationship_with_source_sent_data = relationship_with_source_sent_data.assign(**{\n",
    "    'relationship_word_source_is_person' : relationship_with_source_sent_data.apply(lambda x: is_word_a_person(x.loc['relationship_word_source'].text, lang=x.loc['lang']), axis=1)\n",
    "})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "source": [
    "relationship_with_person_source_sent_data = relationship_with_source_sent_data[relationship_with_source_sent_data.loc[:, 'relationship_word_source_is_person']]\n",
    "display(relationship_with_person_source_sent_data.loc[:, ['sent', 'relationship_word', 'relationship_word_source']].head(5))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same test as before: test split in male/female relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "source": [
    "relationship_word_gender_counts = relationship_with_person_source_sent_data.groupby('relationship_word_gender').apply(lambda x: x.loc[:, 'relationship_word_source_gender'].value_counts()).reset_index().pivot(index='relationship_word_gender', columns=['level_1'], values='relationship_word_source_gender').sort_index()\n",
    "display(relationship_with_person_source_sent_data.groupby('relationship_word_gender').apply(lambda x: x.loc[:, 'relationship_word_source_gender'].value_counts() / x.loc[:, 'relationship_word_source_gender'].value_counts().sum()).reset_index().pivot(index='relationship_word_gender', columns=['level_1'], values='relationship_word_source_gender').sort_index())\n",
    "## test significance\n",
    "from scipy.stats import chi2_contingency\n",
    "test_stat, p_val, dof, expected = chi2_contingency(relationship_word_gender_counts)\n",
    "display(relationship_word_gender_counts)\n",
    "print(test_stat, p_val, dof, relationship_word_gender_counts.sum().sum())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! This is definitely more of the split that I expected. Let's look at some examples of different relationship words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "source": [
    "for relationship_word_i, data_i in relationship_with_person_source_sent_data.groupby('relationship_word'):\n",
    "    print(f'*** relationship word = {relationship_word_i} ***')\n",
    "    relationship_word_gender_i = data_i.loc[:, 'relationship_word_gender'].iloc[0]\n",
    "    for gender_j, data_j in data_i.groupby('relationship_word_source_gender'):\n",
    "        if(gender_j == relationship_word_gender_i):\n",
    "            print(f'** gender = same-gender **')\n",
    "        else:\n",
    "            print(f'** gender = diff-gender **')\n",
    "        display(data_j.loc[:, ['sent', 'relationship_word_source', 'relationship_word_source_gender']].head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare relationship differences by language\n",
    "Now that we've collected data from all languages, let's get the same stats for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "langs = ['es', 'fr', 'it']\n",
    "relationship_sent_data = []\n",
    "relationship_sent_data_dir = 'data/wiki/'\n",
    "for lang_i in langs:\n",
    "    relationship_sent_data_file_i = os.path.join(relationship_sent_data_dir, f'lang={lang_i}_relationship_words_with_source_sent_data.gz')\n",
    "    relationship_sent_data_i = pd.read_csv(relationship_sent_data_file_i, sep='\\t')\n",
    "    relationship_sent_data_i = relationship_sent_data_i.assign(**{\n",
    "        'lang' : lang_i\n",
    "    })\n",
    "    relationship_sent_data.append(relationship_sent_data_i)\n",
    "relationship_sent_data = pd.concat(relationship_sent_data, axis=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "## test diffs w/ chi-2\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "from scipy.stats import chi2_contingency\n",
    "for lang_i, data_i in relationship_sent_data.groupby('lang'):\n",
    "    print(f'lang={lang_i}')\n",
    "    gender_counts_i = data_i.groupby('relationship_word_gender').apply(lambda x: x.loc[:, 'relationship_word_source_gender'].value_counts()).reset_index().pivot(index='relationship_word_gender', columns=['level_1'], values='relationship_word_source_gender').sort_index()\n",
    "    N_i = data_i.shape[0]\n",
    "    # norm per-row\n",
    "#     display(data_i.groupby('relationship_word_gender').apply(lambda x: x.loc[:, 'relationship_word_source_gender'].value_counts() / x.loc[:, 'relationship_word_source_gender'].value_counts().sum()).reset_index().pivot(index='relationship_word_gender', columns=['level_1'], values='relationship_word_source_gender').sort_index())\n",
    "    # norm total\n",
    "    # I am confusion\n",
    "#     for gender_j, data_j in data_i.groupby('relationship_word_gender'):\n",
    "#         print(f'relationship word gender={gender_j}')\n",
    "#         print(data_j.loc[:, 'relationship_word_source_gender'].value_counts() / N_i)\n",
    "#         pass\n",
    "    gender_pct_i = data_i.groupby('relationship_word_gender').apply(lambda x: x.loc[:, 'relationship_word_source_gender'].value_counts() / N_i).reset_index().pivot(index='relationship_word_gender', columns=['level_1'], values='relationship_word_source_gender').sort_index() * 100\n",
    "    display(gender_pct_i)\n",
    "    # row = relationship word gender, col = target gender\n",
    "    ## test significance\n",
    "    test_stat, p_val, dof, expected = chi2_contingency(gender_counts_i)\n",
    "    print(f'X2={test_stat} (p={p_val}, dof={dof}, N={gender_counts_i.sum().sum()}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier\n",
    "Training code that we can't run on the LIT server because of memory problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "device_id = 1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(device_id)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## load sentence data\n",
    "from data_helpers import load_clean_relationship_sent_data\n",
    "langs = ['es', 'fr', 'it']\n",
    "relationship_sent_data = load_clean_relationship_sent_data(langs=langs)\n",
    "display(relationship_sent_data.head())\n",
    "it_relationship_sent_data = relationship_sent_data[relationship_sent_data.loc[:, 'lang']=='it']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "## organize data\n",
    "from transformers import MBartTokenizer, MBartForSequenceClassification\n",
    "from datasets.arrow_dataset import Dataset\n",
    "# model_name = 'facebook/mbart-large' # too big??\n",
    "# model_name = 'sshleifer/tiny-mbart' # too small, doesn't learn anything?\n",
    "model_name = 'facebook/mbart-large-cc25'\n",
    "# tokenizer = MBart50Tokenizer.from_pretrained(model_name)\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "# debug: try w/ only Italian data\n",
    "# max_length = it_relationship_sent_data.loc[:, 'sent'].apply()\n",
    "max_length = 48\n",
    "input_data = tokenizer.batch_encode_plus(it_relationship_sent_data.loc[:, 'sent'], max_length=max_length, truncation=True)\n",
    "# add labels\n",
    "input_data['labels'] = (it_relationship_sent_data.loc[:, 'relationship_type']=='same_gender').astype(int)\n",
    "input_data = Dataset.from_dict(input_data)\n",
    "input_data.set_format(columns=['input_ids', 'attention_mask', 'labels'], type='torch')\n",
    "# split train/test\n",
    "split_input_data = input_data.train_test_split(train_size=0.9, seed=123)\n",
    "train_data = split_input_data['train']\n",
    "test_data = split_input_data['test']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## train!!\n",
    "output_dir = 'relationship_type_classifier/'\n",
    "model = MBartForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model = model.to(torch.cuda.current_device())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## train etc.\n",
    "from datasets import load_metric\n",
    "from transformers import TrainingArguments, Trainer\n",
    "out_dir = 'data/sentence_relationship_gender_classifier/'\n",
    "batch_size = 1\n",
    "num_train_epochs = 3\n",
    "training_args = TrainingArguments(\n",
    "    out_dir,\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=num_train_epochs\n",
    ")\n",
    "compute_metric = load_metric('f1')\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## test lol\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_data_output = [model(**{k : v.unsqueeze(0).to(torch.cuda.current_device()) for k,v in x.items()}) for x  in tqdm(test_data)]\n",
    "test_data_output_logits = torch.vstack([x['logits'] for x in test_data_output])\n",
    "## TODO: is this the best way to convert logits to labels?? range is [-inf, +inf]\n",
    "# test_data_output_logit_labels = (test_data_output_logits > 0.).to(int)\n",
    "test_data_output_logit_labels = test_data_output_logits.argmax(axis=1).cpu()\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "test_data_labels = test_data['labels'].to(int)\n",
    "auc_overall = roc_auc_score(test_data_labels,\n",
    "                            test_data_output_logit_labels)\n",
    "f1_overall = f1_score(test_data_output_logit_labels,\n",
    "                      test_data_labels,\n",
    "                      average='macro')\n",
    "print(auc_overall, f1_overall)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to classify these sentences by relationship type using the classifier trained on the same data as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "## set GPU\n",
    "import os\n",
    "device_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(device_id)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "from data_helpers import load_multilingual_tokenizer\n",
    "from transformers import MBartForSequenceClassification\n",
    "import torch\n",
    "lang = 'it'\n",
    "tokenizer = load_multilingual_tokenizer(tgt_lang_token=lang)\n",
    "trained_model_file = 'relationship_type_classifier/checkpoint-2000/'\n",
    "model = MBartForSequenceClassification.from_pretrained(trained_model_file)\n",
    "model = model.to(torch.cuda.current_device())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "## process data\n",
    "from datasets.arrow_dataset import Dataset\n",
    "max_length = 1024\n",
    "relationship_input_data = tokenizer.batch_encode_plus(relationship_sents.loc[:, 'sent'].values, max_length=max_length, truncation=True)\n",
    "relationship_input_data = Dataset.from_dict(relationship_input_data)\n",
    "relationship_input_data.set_format(columns=['input_ids', 'attention_mask'], type='torch')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## predict\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "relationship_output_data = []\n",
    "with torch.no_grad():\n",
    "    for x in tqdm(relationship_input_data):\n",
    "        x_output = model(**{k : v.unsqueeze(0).to(torch.cuda.current_device()) for k,v in x.items()})\n",
    "        relationship_output_data.append(x_output.logits.view(-1).cpu().numpy())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## combine\n",
    "relationship_output_df = pd.DataFrame(relationship_output_data)\n",
    "# normalize probabilities\n",
    "from scipy.special import logsumexp\n",
    "relationship_output_data = relationship_output_df.apply(lambda x: np.exp(x - logsumexp(x)), axis=1)\n",
    "relationship_output_df = relationship_output_df.assign(**{\n",
    "    'class_label' : relationship_output_df.apply(lambda x: x.argmax(), axis=1),\n",
    "    'class_label_prob' : relationship_output_df.apply(lambda x: x.max(), axis=1),\n",
    "}).drop([0,1], axis=1)\n",
    "# fix class names\n",
    "class_label_name_lookup = {\n",
    "    0 : 'diff_gender',\n",
    "    1 : 'same_gender',\n",
    "}\n",
    "relationship_output_df = relationship_output_df.assign(**{\n",
    "    'class_label' : relationship_output_df.loc[:, 'class_label'].apply(class_label_name_lookup.get)\n",
    "})\n",
    "print(relationship_output_df.loc[:, 'class_label'].value_counts())\n",
    "## recombine w/ sents\n",
    "relationship_sent_output_data = pd.concat([\n",
    "    relationship_output_df,\n",
    "    relationship_sents,\n",
    "], axis=1)\n",
    "display(relationship_sent_output_data.head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the predictions accurate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for label_i, data_i in relationship_sent_output_data.groupby('class_label'):\n",
    "    print(f'example sents for label={label_i}')\n",
    "    data_i.sort_values('class_label_prob', ascending=False, inplace=True)\n",
    "    display(data_i.loc[:, ['relationship_word', 'sent', 'class_label_prob']].head(20))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! Not great. The model seemed to learn about coccurrence of nouns with the same/different genders, but not about actual relationships.\n",
    "\n",
    "- `diff_gender`\n",
    "    - (IT) `**la signora** nance precedette **suo marito** nella morte.` (EN) `Mrs. Nance preceded her husband in death.`\n",
    "- `same_gender`\n",
    "    - (IT) `**la coppia** conosce anche peppino e **la sua fidanzata** katrine che successivamente si fidanza con mirko.` (EN) `the couple also meets peppino and his girlfriend katrine who later gets engaged to mirko.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "## save for posterity\n",
    "relationship_sent_output_data.to_csv('data/wiki/relationship_sent_pred_output_data.gz', sep='\\t', compression='gzip', index=False)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
